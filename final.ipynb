{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuyang Li, “INTERVIEW: NPR Media Dialog Transcripts.” Kaggle, doi: 10.34740/KAGGLE/DS/590180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/nanamathis/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nanamathis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nanamathis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from altair import Chart, X, Y, Color, Scale\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import wordcloud\n",
    "import textatistic\n",
    "import seaborn as sbn\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords') \n",
    "nltk.download('punkt')\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data\n",
    "#### IGNORE THIS RN - If you want to skip reading, cleaning, and merging, go to the header labeled: [Download Ready To Go Data](#Download-Ready-To-Go-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First reading the .csv files:\n",
    "- episodes: information on each episode by id, includes the program it's under, title, and date which is parsed as a date\n",
    "- utter2: record of episode content when there are only 2 people in speaking in the episode\n",
    "- utter: record of episode content when there are any number of people in speaking in the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = pd.read_csv('interview-npr-media-dialog-transcripts/episodes.csv',parse_dates=['episode_date'])\n",
    "# utter2 = pd.read_csv('interview-npr-media-dialog-transcripts/utterances-2sp.csv')\n",
    "utter = pd.read_csv('interview-npr-media-dialog-transcripts/utterances.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now reading the .json files:\n",
    "- host_ids: host names with their id number\n",
    "- host_map: by host id, includes name, list of episodes id's, and list of programs \n",
    "- test_train_valid: the data split into train, test, validate for any modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_ids = pd.read_json('interview-npr-media-dialog-transcripts/host_id.json',orient='index')\n",
    "host_map = pd.read_json('interview-npr-media-dialog-transcripts/host-map.json',orient='index')\n",
    "test_train_valid = pd.read_json('interview-npr-media-dialog-transcripts/splits-ns2.json',orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data\n",
    "Right now we don't need the train_test_valid dataset so we will leave it alone. We will be cleaning and organizing the rest of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the episodes dataset we will rename 'id' to be 'episode_id' to be clear since the hosts have ids as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>program</th>\n",
       "      <th>title</th>\n",
       "      <th>episode_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>98814</td>\n",
       "      <td>Morning Edition</td>\n",
       "      <td>Senate Ushers In New Year With 'Fiscal Cliff' ...</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>98824</td>\n",
       "      <td>Morning Edition</td>\n",
       "      <td>Cheap Bubbly Or Expensive Sparkling Wine? Look...</td>\n",
       "      <td>2012-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>98821</td>\n",
       "      <td>Morning Edition</td>\n",
       "      <td>U.S. Gas Prices Reach Record Level In 2012</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>98806</td>\n",
       "      <td>Morning Edition</td>\n",
       "      <td>House Approves 'Fiscal Cliff' Measure</td>\n",
       "      <td>2013-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>98823</td>\n",
       "      <td>Morning Edition</td>\n",
       "      <td>The Fiscal Cliff: A Love Story</td>\n",
       "      <td>2012-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_id          program  \\\n",
       "0       98814  Morning Edition   \n",
       "1       98824  Morning Edition   \n",
       "2       98821  Morning Edition   \n",
       "3       98806  Morning Edition   \n",
       "4       98823  Morning Edition   \n",
       "\n",
       "                                               title episode_date  \n",
       "0  Senate Ushers In New Year With 'Fiscal Cliff' ...   2013-01-01  \n",
       "1  Cheap Bubbly Or Expensive Sparkling Wine? Look...   2012-12-31  \n",
       "2         U.S. Gas Prices Reach Record Level In 2012   2013-01-01  \n",
       "3              House Approves 'Fiscal Cliff' Measure   2013-01-02  \n",
       "4                     The Fiscal Cliff: A Love Story   2012-12-31  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes.rename(columns={'id':'episode_id'},inplace=True)\n",
    "# there's only one episodes in 1999 and we will use the id to remove it from the other data\n",
    "nineteen99 = episodes[episodes['episode_date'].dt.year == 1999]\n",
    "# removing episodes during 1999 since there's only one and the years jump from \n",
    "# 1999 to 2004 and then it's consecutive by year from 2004 until 2019\n",
    "episodes = episodes[~(episodes['episode_date'].dt.year == 1999)]\n",
    "episodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know if I'll use the utter2 dataset since the utter dataset covers the episodes in it as well but we will change the name of the column 'id' to 'episode_id' to match the other dataset variables. We will also clean the text of what they said so it's all uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter.rename(columns={'episode':'episode_id'},inplace=True)\n",
    "# utter2.rename(columns={'episode':'episode_id'},inplace=True)\n",
    "# remove all data from the one episode that took place in 1999 with the episode_id of 141179\n",
    "utter = utter[~(utter['episode_id'] == nineteen99.iloc[0]['episode_id'])]\n",
    "# utter2 = utter2[~(utter2['episode_id'] == nineteen99.iloc[0]['episode_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all text to lower case\n",
    "utter['utterance'] = utter.utterance.str.lower()\n",
    "# utter2['utterance'] = utter2.utterance.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove strange character\n",
    "utter['utterance'] = utter.utterance.str.replace('\\ufeff','') \n",
    "# utter2['utterance'] = utter2.utterance.str.replace('\\ufeff','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate whitespace from beginning and end\n",
    "utter['utterance'] = utter.utterance.str.strip() \n",
    "# utter2['utterance'] = utter2.utterance.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up speaker column to make all host names be lower case as well as\n",
    "# remove the text \", host\" so it will match with the hosts datasets\n",
    "utter['speaker'] = utter.speaker.str.lower()\n",
    "utter['speaker'] = utter.speaker.str.replace(', host','',regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to clean the text even more! It might have been more efficient to do this before I split and grouped the data in the previous two cells but I'm not sure due to the format it was in as a list of sentences rather than a string. I might try it later but for now we will do it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacePunct(S):\n",
    "    \"\"\"\n",
    "    Replaces punctuation with whitespace, eliminating \n",
    "    punctuation in a string. Returns the updated string.\n",
    "    input:\n",
    "    S (string) - a string that has punctuation characters\n",
    "    output:\n",
    "    (string) - new string with punctuation replaced with a space\n",
    "    \"\"\" \n",
    "    return str(S).translate(str.maketrans(\n",
    "        string.punctuation, ' '*len(string.punctuation)))\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "# def preprocess(S):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     return [word for word in nltk.word_tokenize(text) \n",
    "#             if word not in sw and not word.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# replace punctuation with spaces\n",
    "utter['utterance'] = utter.utterance.apply(replacePunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the text into a list of words/strings\n",
    "# add a new column, called utter_list, to our dataframe,\n",
    "# by applying the function nltk.word_tokenize to the text column.\n",
    "utter['utter_list'] = utter.utterance.apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stopwords, i.e. is, am, etc.\n",
    "\n",
    "# this keeps only regular English words and removes common words such as is, am, etc.\n",
    "utter['utter_list'] = utter.utter_list.apply(\n",
    "    lambda x: [y for y in x if y not in sw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up host_map so it makes more sense and organized. Also rename the name column to be speaker so that we can merge it with the episodes dataset. We don't really need this since we create basically the same thing later on but in synch with the rest of the data lists..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the index actually corresponds to the host_id so we are\n",
    "# sorting by index and then reseting the index to be a column\n",
    "host_map.sort_index(axis=0,inplace=True)\n",
    "host_map.reset_index(inplace=True)\n",
    "# renaming index column to be host_id and reordering the columns\n",
    "host_map.rename(columns={'index':'host_id','name':'speaker'},inplace=True)\n",
    "host_map = host_map[['speaker','host_id','episodes','programs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean up names for hosts, it's an issue in both host_ids and host_map\n",
    "# there are double counts like \"text melissa block\" and \"melissa block\" and\n",
    "# \"mr. neal conan\" vs. \"neal conan\"\n",
    "# some aren't even included in the host data, i.e. mara liasson, byline is listed as guest\n",
    "# x = host_map.name.str.extractall('(global)(\\s\\w+)',flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up host_ids to be sorted by their id, we also renamed the name column to be speaker so that we can merge it with the episodes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "host_ids.sort_values(by=0,ascending=True,inplace=True)\n",
    "host_ids.reset_index(inplace=True)\n",
    "host_ids.rename(columns={'index':'speaker',0:'host_id'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epScript is a dataframe merging episodes and utter so that we have all the information of an episode and the script. We will also add the host_id to this frame and fill guest speakers' id or any unfilled values as -1. Then we will sort the dataframe by episode_id following episode order. Then we sort the episodes by date and if there are multiple episodes on a day, by their episode_id and also the order of the script, episode_order. We will also keep track of the number of words in a given utterance with 'word_count.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>episode_order</th>\n",
       "      <th>speaker</th>\n",
       "      <th>utterance</th>\n",
       "      <th>utter_list</th>\n",
       "      <th>program</th>\n",
       "      <th>title</th>\n",
       "      <th>episode_date</th>\n",
       "      <th>host_id</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>85414</td>\n",
       "      <td>0</td>\n",
       "      <td>liane hansen</td>\n",
       "      <td>on friday  cia director george tenet announced...</td>\n",
       "      <td>[friday, cia, director, george, tenet, announc...</td>\n",
       "      <td>Weekend Edition Sunday</td>\n",
       "      <td>Iraq WMD Questioned</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>117.0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>85414</td>\n",
       "      <td>1</td>\n",
       "      <td>david kay</td>\n",
       "      <td>good morning  i m happy to be with you   hanse...</td>\n",
       "      <td>[good, morning, happy, hansen, since, deliveri...</td>\n",
       "      <td>Weekend Edition Sunday</td>\n",
       "      <td>Iraq WMD Questioned</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>85414</td>\n",
       "      <td>2</td>\n",
       "      <td>liane hansen</td>\n",
       "      <td>so prior to last year s invasion and your repo...</td>\n",
       "      <td>[prior, last, year, invasion, report, october,...</td>\n",
       "      <td>Weekend Edition Sunday</td>\n",
       "      <td>Iraq WMD Questioned</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>117.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>85414</td>\n",
       "      <td>3</td>\n",
       "      <td>david kay</td>\n",
       "      <td>not very much   i think that s true</td>\n",
       "      <td>[much, think, true]</td>\n",
       "      <td>Weekend Edition Sunday</td>\n",
       "      <td>Iraq WMD Questioned</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>85414</td>\n",
       "      <td>4</td>\n",
       "      <td>liane hansen</td>\n",
       "      <td>have you determined that you re never going to...</td>\n",
       "      <td>[determined, never, going, find, clear, eviden...</td>\n",
       "      <td>Weekend Edition Sunday</td>\n",
       "      <td>Iraq WMD Questioned</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>117.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_id  episode_order       speaker  \\\n",
       "0       85414              0  liane hansen   \n",
       "1       85414              1     david kay   \n",
       "2       85414              2  liane hansen   \n",
       "3       85414              3     david kay   \n",
       "4       85414              4  liane hansen   \n",
       "\n",
       "                                           utterance  \\\n",
       "0  on friday  cia director george tenet announced...   \n",
       "1  good morning  i m happy to be with you   hanse...   \n",
       "2  so prior to last year s invasion and your repo...   \n",
       "3               not very much   i think that s true    \n",
       "4  have you determined that you re never going to...   \n",
       "\n",
       "                                          utter_list                 program  \\\n",
       "0  [friday, cia, director, george, tenet, announc...  Weekend Edition Sunday   \n",
       "1  [good, morning, happy, hansen, since, deliveri...  Weekend Edition Sunday   \n",
       "2  [prior, last, year, invasion, report, october,...  Weekend Edition Sunday   \n",
       "3                                [much, think, true]  Weekend Edition Sunday   \n",
       "4  [determined, never, going, find, clear, eviden...  Weekend Edition Sunday   \n",
       "\n",
       "                 title episode_date  host_id  word_count  \n",
       "0  Iraq WMD Questioned   2004-01-25    117.0          88  \n",
       "1  Iraq WMD Questioned   2004-01-25     -1.0          72  \n",
       "2  Iraq WMD Questioned   2004-01-25    117.0           9  \n",
       "3  Iraq WMD Questioned   2004-01-25     -1.0           3  \n",
       "4  Iraq WMD Questioned   2004-01-25    117.0           9  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epScript = pd.merge(utter,episodes,on='episode_id',how='left')\n",
    "epScript = pd.merge(epScript,host_ids,on='speaker',how=\"left\")\n",
    "epScript.fillna(-1,inplace=True)\n",
    "epScript.sort_values(by=['episode_date','episode_id','episode_order'],inplace=True,ascending=True)\n",
    "epScript.reset_index(drop=True,inplace=True)\n",
    "epScript['word_count'] = epScript.utter_list.str.len()\n",
    "epScript.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I found out that the way I aggregate the utterances together to be a huge list of words, there is an extra \n",
    "# [ and ' in the string of the first index and ' and ] in the string of the last index.\n",
    "def remExtraBrackets(L):\n",
    "    \"\"\"\n",
    "    Removes the extra apostrophes and brackets \n",
    "    at the first and last index of a list. \n",
    "    Returns the updated list as a string.\n",
    "    input:\n",
    "    L (list) - a list of strings with the first and last elements having 2 extra characters in the front and back respectively\n",
    "    output:\n",
    "    (string) - string form of the changed list\n",
    "    \"\"\" \n",
    "    L[0] = L[0][2:]\n",
    "    L[-1] = L[-1][:-2]\n",
    "    # return a string because later we will make a list of the important words\n",
    "    return ' '.join(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataframe, byEpisode, that has been grouped by the episode_id and the non-repeating valued columns will then be concatenated to a list. We know that the index of one list, say at index 0 in host_id, correlates to the speaker name at index 0. We also will add a row to keep the total count of words said in a given episode with 'total_count.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>program</th>\n",
       "      <th>title</th>\n",
       "      <th>episode_date</th>\n",
       "      <th>speaker</th>\n",
       "      <th>utterance</th>\n",
       "      <th>utter_list</th>\n",
       "      <th>host_id</th>\n",
       "      <th>word_count</th>\n",
       "      <th>total_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>85414</td>\n",
       "      <td>Weekend Edition Sunday</td>\n",
       "      <td>Iraq WMD Questioned</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>[liane hansen, david kay, liane hansen, david ...</td>\n",
       "      <td>[on friday  cia director george tenet announce...</td>\n",
       "      <td>[[friday, cia, director, george, tenet, announ...</td>\n",
       "      <td>[117.0, -1.0, 117.0, -1.0, 117.0, -1.0, 117.0,...</td>\n",
       "      <td>[88, 72, 9, 3, 9, 64, 7, 38, 12, 50, 13, 13, 3...</td>\n",
       "      <td>1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>85620</td>\n",
       "      <td>Weekend Edition Saturday</td>\n",
       "      <td>Iraqis Vote for Local Council</td>\n",
       "      <td>2004-02-14</td>\n",
       "      <td>[scott simon, scott simon, emily harris report...</td>\n",
       "      <td>[in iraq earlier today  guerrillas attacked a ...</td>\n",
       "      <td>[[iraq, earlier, today, guerrillas, attacked, ...</td>\n",
       "      <td>[12.0, 12.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>[28, 58, 11, 2, 3, 31, 17, 48, 24, 38, 3, 25, ...</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>135286</td>\n",
       "      <td>Morning Edition</td>\n",
       "      <td>Study Sheds Light on Compulsive Hoarding</td>\n",
       "      <td>2004-06-07</td>\n",
       "      <td>[steve inskeep, michelle trudeau reporting, ri...</td>\n",
       "      <td>[a new study focuses on the brains of compulsi...</td>\n",
       "      <td>[[new, study, focuses, brains, compulsive, hoa...</td>\n",
       "      <td>[16.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>[31, 18, 19, 37, 5, 4, 13, 19, 11, 4, 16, 11, ...</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>134619</td>\n",
       "      <td>Morning Edition</td>\n",
       "      <td>Toni Morrison's 'Good' Ghosts</td>\n",
       "      <td>2004-09-20</td>\n",
       "      <td>[_no_speaker, steve inskeep, steve inskeep, st...</td>\n",
       "      <td>[nan, nan, this is morning edition from npr ne...</td>\n",
       "      <td>[[nan], [nan], [morning, edition, npr, news, s...</td>\n",
       "      <td>[-1.0, 16.0, 16.0, 16.0, 16.0, 16.0, 5.0, 5.0,...</td>\n",
       "      <td>[1, 1, 6, 1, 33, 1, 1, 34, 1, 19, 1, 30, 1, 2,...</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>84651</td>\n",
       "      <td>Talk of the Nation</td>\n",
       "      <td>Calls for States' Rights, from the Left</td>\n",
       "      <td>2004-11-30</td>\n",
       "      <td>[neal conan, neal conan, neal conan, neal cona...</td>\n",
       "      <td>[this is talk of the nation   i m neal conan i...</td>\n",
       "      <td>[[talk, nation, neal, conan, washington], [yea...</td>\n",
       "      <td>[7.0, 7.0, 7.0, 7.0, 7.0, 7.0, -1.0, 7.0, -1.0...</td>\n",
       "      <td>[5, 75, 41, 39, 16, 3, 2, 20, 34, 20, 30, 4, 4...</td>\n",
       "      <td>3375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_id                   program  \\\n",
       "0       85414    Weekend Edition Sunday   \n",
       "1       85620  Weekend Edition Saturday   \n",
       "2      135286           Morning Edition   \n",
       "3      134619           Morning Edition   \n",
       "4       84651        Talk of the Nation   \n",
       "\n",
       "                                      title episode_date  \\\n",
       "0                       Iraq WMD Questioned   2004-01-25   \n",
       "1             Iraqis Vote for Local Council   2004-02-14   \n",
       "2  Study Sheds Light on Compulsive Hoarding   2004-06-07   \n",
       "3             Toni Morrison's 'Good' Ghosts   2004-09-20   \n",
       "4   Calls for States' Rights, from the Left   2004-11-30   \n",
       "\n",
       "                                             speaker  \\\n",
       "0  [liane hansen, david kay, liane hansen, david ...   \n",
       "1  [scott simon, scott simon, emily harris report...   \n",
       "2  [steve inskeep, michelle trudeau reporting, ri...   \n",
       "3  [_no_speaker, steve inskeep, steve inskeep, st...   \n",
       "4  [neal conan, neal conan, neal conan, neal cona...   \n",
       "\n",
       "                                           utterance  \\\n",
       "0  [on friday  cia director george tenet announce...   \n",
       "1  [in iraq earlier today  guerrillas attacked a ...   \n",
       "2  [a new study focuses on the brains of compulsi...   \n",
       "3  [nan, nan, this is morning edition from npr ne...   \n",
       "4  [this is talk of the nation   i m neal conan i...   \n",
       "\n",
       "                                          utter_list  \\\n",
       "0  [[friday, cia, director, george, tenet, announ...   \n",
       "1  [[iraq, earlier, today, guerrillas, attacked, ...   \n",
       "2  [[new, study, focuses, brains, compulsive, hoa...   \n",
       "3  [[nan], [nan], [morning, edition, npr, news, s...   \n",
       "4  [[talk, nation, neal, conan, washington], [yea...   \n",
       "\n",
       "                                             host_id  \\\n",
       "0  [117.0, -1.0, 117.0, -1.0, 117.0, -1.0, 117.0,...   \n",
       "1  [12.0, 12.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1....   \n",
       "2  [16.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1....   \n",
       "3  [-1.0, 16.0, 16.0, 16.0, 16.0, 16.0, 5.0, 5.0,...   \n",
       "4  [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, -1.0, 7.0, -1.0...   \n",
       "\n",
       "                                          word_count  total_count  \n",
       "0  [88, 72, 9, 3, 9, 64, 7, 38, 12, 50, 13, 13, 3...         1197  \n",
       "1  [28, 58, 11, 2, 3, 31, 17, 48, 24, 38, 3, 25, ...          454  \n",
       "2  [31, 18, 19, 37, 5, 4, 13, 19, 11, 4, 16, 11, ...          650  \n",
       "3  [1, 1, 6, 1, 33, 1, 1, 34, 1, 19, 1, 30, 1, 2,...          627  \n",
       "4  [5, 75, 41, 39, 16, 3, 2, 20, 34, 20, 30, 4, 4...         3375  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byEpisode = epScript.groupby(['episode_id','program','title','episode_date']\n",
    "                             )['speaker','utterance','utter_list','host_id','word_count'].agg(list).reset_index()\n",
    "byEpisode.sort_values(by=['episode_date','episode_id'],inplace=True,ascending=True)\n",
    "byEpisode.reset_index(drop=True,inplace=True)\n",
    "byEpisode['total_count'] = byEpisode.word_count.apply(lambda l: np.nansum(l))\n",
    "byEpisode.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataframe, byHost, that has been grouped by the speaker who is a host (has a host_id of not -1) and the non-repeating valued columns will then be concatenated to a list. We know that the index of one list, say at index 0 in episode_id, correlates to the program at index 0. We also will add a row to keep the total count of words said in a given episode with 'total_count.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we already know non hosts have a host_id of -1 since we set that above\n",
    "# byHost = epScript[~(epScript['host_id'] == -1)].groupby(['speaker','host_id']\n",
    "#                                                        )['episode_id','episode_order','utterance','utter_list',\n",
    "#                                                          'program','title','episode_date','word_count'\n",
    "#                                                         ].agg(list).reset_index()\n",
    "# byHost.sort_values(by=['speaker','host_id'],inplace=True,ascending=True)\n",
    "# byHost.reset_index(drop=True,inplace=True)\n",
    "# byHost['total_count'] = byHost.word_count.apply(lambda l: np.nansum(l))\n",
    "# byHost.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping the episodes by year where utterance has all scripts from that year as a string and avg_epCount is the average length of an episode that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_date</th>\n",
       "      <th>utterance</th>\n",
       "      <th>utter_list</th>\n",
       "      <th>avg_epCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>on friday cia director george tenet announced ...</td>\n",
       "      <td>[friday, cia, director, george, tenet, announc...</td>\n",
       "      <td>1064.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>this is talk of the nation i m neal conan in w...</td>\n",
       "      <td>[talk, nation, neal, conan, washington, month,...</td>\n",
       "      <td>587.587842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>from npr news this is weekend edition i m lian...</td>\n",
       "      <td>[npr, news, weekend, edition, liane, hansen, v...</td>\n",
       "      <td>577.478093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>from npr news this is news notes i m farai chi...</td>\n",
       "      <td>[npr, news, news, notes, farai, chideya, outse...</td>\n",
       "      <td>570.710231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>from npr news this is news notes i m farai chi...</td>\n",
       "      <td>[npr, news, news, notes, farai, chideya, first...</td>\n",
       "      <td>588.163440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_date                                          utterance  \\\n",
       "0          2004  on friday cia director george tenet announced ...   \n",
       "1          2005  this is talk of the nation i m neal conan in w...   \n",
       "2          2006  from npr news this is weekend edition i m lian...   \n",
       "3          2007  from npr news this is news notes i m farai chi...   \n",
       "4          2008  from npr news this is news notes i m farai chi...   \n",
       "\n",
       "                                          utter_list  avg_epCount  \n",
       "0  [friday, cia, director, george, tenet, announc...  1064.285714  \n",
       "1  [talk, nation, neal, conan, washington, month,...   587.587842  \n",
       "2  [npr, news, weekend, edition, liane, hansen, v...   577.478093  \n",
       "3  [npr, news, news, notes, farai, chideya, outse...   570.710231  \n",
       "4  [npr, news, news, notes, farai, chideya, first...   588.163440  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gathering text by year\n",
    "textByYear = byEpisode.groupby(byEpisode['episode_date'].dt.year\n",
    "                               ).agg({'utterance': lambda l:\n",
    "                                      [word for sentence in l for word in str(sentence).split()],\n",
    "                                      'utter_list': lambda l: \n",
    "                                        [item for sublist in l for item in sublist],\n",
    "                                      'total_count': 'mean',\n",
    "                                     }).reset_index()\n",
    "textByYear['utterance'] = textByYear.utterance.apply(remExtraBrackets)\n",
    "textByYear['utter_list'] = textByYear.utter_list.apply(lambda l: [item for sublist in l for item in sublist])\n",
    "textByYear.rename(columns={'total_count':'avg_epCount'},inplace=True)\n",
    "textByYear.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping the episodes by program where utterance has all scripts from that program as a string and avg_epCount is the average length of an episode for that program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textByProg = byEpisode.groupby(byEpisode['program']\n",
    "#                                ).agg({'utterance': lambda l:\n",
    "#                                       [word for sentence in l for word in str(sentence).split()],\n",
    "#                                       'utter_list': lambda l: \n",
    "#                                         [item for sublist in l for item in sublist],\n",
    "#                                       'total_count': 'mean',\n",
    "#                                      }).reset_index()\n",
    "# textByProg['utterance'] = textByProg.utterance.apply(remExtraBrackets)\n",
    "# textByProg['utter_list'] = textByProg.utter_list.apply(lambda l: [item for sublist in l for item in sublist])\n",
    "# textByProg.rename(columns={'total_count':'avg_epCount'},inplace=True)\n",
    "\n",
    "# textByProg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the episodes by host where utterance has all scripts from that host as a string and avg_epCount is the average length of an episode for that host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textByHost = byHost.groupby(byHost['speaker']\n",
    "#                                ).agg({'utterance': lambda l:\n",
    "#                                         [word for sentence in l for word in str(sentence).split()],\n",
    "#                                       'utter_list': lambda l: \n",
    "#                                         [item for sublist in l for item in sublist],\n",
    "#                                       'total_count': 'mean',\n",
    "#                                      }).reset_index()\n",
    "# textByHost['utterance'] = textByHost.utterance.apply(remExtraBrackets)\n",
    "# textByHost['utter_list'] = textByHost.utter_list.apply(lambda l: [item for sublist in l for item in sublist])\n",
    "# textByHost.rename(columns={'total_count':'avg_epCount'},inplace=True)\n",
    "# textByHost.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE - Download Ready To Go Data\n",
    "You can use this to get the DataFrames without doing the work above. First run the cell below and then if you want to skip more cleaning and prepping then you can skip to: [Download More Ready To Go Data](#Download-More-Ready-To-Go-Data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epScript = pd.read_csv('from-notebook/transcripts-divided.csv',parse_dates=['episode_date'])\n",
    "# bySpeaker = pd.read_csv('from-notebook/transcripts-by-speaker.csv',parse_dates=['episode_date'])\n",
    "# byEpisode = pd.read_csv('from-notebook/transcripts-by-episode.csv',parse_dates=['episode_date'])\n",
    "# textByYear = pd.read_csv('from-notebook/transcripts-by-year.csv',index_col=0)\n",
    "# textByProg = pd.read_csv('from-notebook/transcripts-by-program.csv',index_col=0)\n",
    "# textByHost = pd.read_csv('from-notebook/transcripts-by-host.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Prepping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions to help with charting and plotting the frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createCounter(df):\n",
    "#     \"\"\"\n",
    "#     Creates and returns a Counter for df on the words in 'utter_list'\n",
    "#     input:\n",
    "#     df (DataFrame) - DataFrame you want to count\n",
    "#     output:\n",
    "#     (Counter) - a Counter that has summed the occurences of each word \n",
    "#     in 'utter_list' of the DataFrame\n",
    "#     \"\"\" \n",
    "#     return Counter(df.utter_list.sum())\n",
    "# def dictOfCounters(df,L):\n",
    "#     for e in L:\n",
    "#         counter = createCounter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearCounters = {}\n",
    "# for year in textByYear.episode_date:\n",
    "#     counter = createCounter(textByYear)\n",
    "#     yearCounters[year] = counter\n",
    "#     print('finished: ',year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progCounters = {}\n",
    "# for program in textByProg.program:\n",
    "#     counter = createCounter(textByProg)\n",
    "#     progCounters[program] = counter\n",
    "#     print('finished: ',program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this doesn't represent the entire dataset since only using a portion of \n",
    "# # the hosts (top 10 hosts that speak the most) and excluding the guest speakers\n",
    "# hostCounters = {}\n",
    "# top10Hosts = textByHost.sort_values(by='avg_epCount',inplace=False,ascending=False)[:10]\n",
    "# for host in top10Hosts.speaker:\n",
    "#     counter = createCounter(textByHost)\n",
    "#     hostCounters[host] = counter\n",
    "#     print('finished: ',host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE - Download More Ready To Go Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('from-notebook/yearCounters.txt', 'r') as file:\n",
    "#     yearCounters = json.load(file)\n",
    "# with open('from-notebook/progCounters.txt', 'r') as file:\n",
    "#     progCounters = json.load(file)\n",
    "# with open('from-notebook/hostCounters.txt', 'r') as file:\n",
    "#     hostCounters = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def toDictOfCounters(dictionary):\n",
    "#     \"\"\"\n",
    "#     Converts a dictionary of dictionarys with words \n",
    "#     and counts and their respective keys and values \n",
    "#     to a dictionary of Counters\n",
    "#     input:\n",
    "#     dictionary (dict) - dictionary of dictionarys \n",
    "#     whose keys and values are words and their counts\n",
    "#     output:\n",
    "#     (dict) - a dictionary of Counters\n",
    "#     \"\"\"\n",
    "#     for key in dictionary.keys():\n",
    "#         dictionary[key] = Counter(dictionary[key])\n",
    "#     return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearCounters = toDictOfCounters(yearCounters)\n",
    "# progCounters = toDictOfCounters(progCounters)\n",
    "# hostCounters = toDictOfCounters(hostCounters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def countTopics(df, topics):\n",
    "#     \"\"\"\n",
    "#     Creates and returns a Counter for df on the words in 'utter_list'\n",
    "#     input:\n",
    "#     df (DataFrame) - DataFrame you want to count\n",
    "#     topics (list) - a list of strings that are topics to count\n",
    "#     output:\n",
    "#     nothing - this function counts the occurences of each string in\n",
    "#     topics in df.utterance column and adds that count to a column \n",
    "#     with the name of the topic. instead of returning a new DataFrame,\n",
    "#     it changes in place the given df\n",
    "#     \"\"\"\n",
    "#     for t in topics:\n",
    "#         df[t] = df.utterance.str.count(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics1 = ['women','mexico','united states',\n",
    "#           'violence', 'health', 'education',\n",
    "#           'border', 'president', 'isis',\n",
    "#           'sexual assault', 'trump', 'terrorism']\n",
    "# topics2 = ['women','politics','united states',\n",
    "#           'violence', 'global warming', 'education',\n",
    "#           'border', 'climate change', 'isis',\n",
    "#           'sexual assault', 'trump', 'terrorism']\n",
    "# topics3 = ['women','men','assault','reproduction',\n",
    "#           'violence', 'education','marriage','trump',\n",
    "#           'sexual assault','love', 'fact','sorry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year_summ = textByYear.groupby('episode_date', as_index=False)['utterance'].sum()\n",
    "# countTopics(year_summ, topics1)\n",
    "# year_summ = year_summ[['episode_date']+topics1]\n",
    "\n",
    "# # Altair works better when data is in wide_form so we can easily \n",
    "# # convert the long form DataFrame using melt().\n",
    "# year_summ = year_summ.melt('episode_date', var_name='word', value_name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Currently looking at the words by years with each having its own graph\n",
    "# alt.Chart(year_summ).mark_line().encode(\n",
    "#     x='episode_date:O',\n",
    "#     y='count:Q',\n",
    "#     color='word:N'\n",
    "# ).properties(\n",
    "#     width=180,\n",
    "#     height=180\n",
    "# ).facet(\n",
    "#     facet='word:N',\n",
    "#     columns=3\n",
    "# ).interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the graphs to show all the words over all the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6. Make a graph to show the frequency with which various topics are discussed over the years. For example, ‘peace’\n",
    "# # is consistently a popular word as is ‘freedom’ and ‘human rights’. What about ‘HIV’ or ‘terrorism’ or ‘global\n",
    "# # warming’. Compare two phrases like ‘global warming’ and ‘climate change’.\n",
    "# yearChart = alt.Chart(year_summ).mark_line().encode(\n",
    "#     x='episode_date:O',\n",
    "#     y='count:Q',\n",
    "#     color='word:N'\n",
    "# )\n",
    "# yearChart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prog_summ = textByProg.groupby('program', as_index=False)['utterance'].sum()\n",
    "# countTopics(prog_summ, topics2)\n",
    "# prog_summ = prog_summ[['program']+topics2]\n",
    "\n",
    "# # Altair works better when data is in wide_form so we can easily \n",
    "# # convert the long form DataFrame using melt().\n",
    "# prog_summ = prog_summ.melt('program', var_name='word', value_name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progChart = alt.Chart(prog_summ).mark_line().encode(\n",
    "#     x='program:N',\n",
    "#     y='count:Q',\n",
    "#     color='word:N'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top10Hosts = textByHost.sort_values(by='avg_epCount',inplace=False,ascending=False)[:10]\n",
    "# host_summ = top10Hosts.groupby('speaker', as_index=False)['utterance'].sum()\n",
    "# countTopics(host_summ, topics3)\n",
    "# host_summ = host_summ[['speaker']+topics3]\n",
    "\n",
    "# # Altair works better when data is in wide_form so we can easily \n",
    "# # convert the long form DataFrame using melt().\n",
    "# host_summ = host_summ.melt('speaker', var_name='word', value_name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hostChart = alt.Chart(host_summ).mark_line().encode(\n",
    "#     x='speaker:N',\n",
    "#     y='count:Q',\n",
    "#     color='word:N'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordClouds reflecting frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make WordCloud from the counter\n",
    "# def createWordCloud(counter):\n",
    "#     import wordcloud\n",
    "#     wc = wordcloud.WordCloud(background_color ='white')\n",
    "#     # generate word cloud\n",
    "#     wc.generate_from_frequencies(frequencies=counter)\n",
    "#     return wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# i=0\n",
    "# for prog in progCounters.keys():\n",
    "#     ax = fig.add_subplot(2,1,i+1)\n",
    "#     wordcloud = createWordCloud(progCounters[prog])\n",
    "#     ax.imshow(wordcloud)\n",
    "#     ax.axis('off')\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     plt.imshow(wc, interpolation=\"bilinear\")\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "# createWordCloud(progCounters['All Things Considered']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charting average words per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgWordsPerEp = byEpisode['total_count'].mean()\n",
    "\n",
    "# I created a dataframe that just has each year and the avergage\n",
    "# length of all speeches called source\n",
    "source = pd.DataFrame({\n",
    "  'year': textByYear.episode_date,\n",
    "  'avg': avgWordsPerEp\n",
    "})\n",
    "# so that I can create a single horizontal graph that I can\n",
    "# overlay it with the byYeargraph!\n",
    "avg = alt.Chart(source).mark_rule(color='red').encode(\n",
    "    y='avg'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-922feae5a3d14725ad2f2371cb936b87\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-922feae5a3d14725ad2f2371cb936b87\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-8b7a012138a6c6024e2b885ed734814c\"}, \"mark\": {\"type\": \"rule\", \"color\": \"red\"}, \"encoding\": {\"y\": {\"type\": \"quantitative\", \"field\": \"avg\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-8b7a012138a6c6024e2b885ed734814c\": [{\"year\": 2004, \"avg\": 520.1681964181893}, {\"year\": 2005, \"avg\": 520.1681964181893}, {\"year\": 2006, \"avg\": 520.1681964181893}, {\"year\": 2007, \"avg\": 520.1681964181893}, {\"year\": 2008, \"avg\": 520.1681964181893}, {\"year\": 2009, \"avg\": 520.1681964181893}, {\"year\": 2010, \"avg\": 520.1681964181893}, {\"year\": 2011, \"avg\": 520.1681964181893}, {\"year\": 2012, \"avg\": 520.1681964181893}, {\"year\": 2013, \"avg\": 520.1681964181893}, {\"year\": 2014, \"avg\": 520.1681964181893}, {\"year\": 2015, \"avg\": 520.1681964181893}, {\"year\": 2016, \"avg\": 520.1681964181893}, {\"year\": 2017, \"avg\": 520.1681964181893}, {\"year\": 2018, \"avg\": 520.1681964181893}, {\"year\": 2019, \"avg\": 520.1681964181893}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(textByYear).mark_bar().encode(\n",
    "    x='episode_date:O',\n",
    "    y='avg_epCount:Q',\n",
    "    color='episode_date:O'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bars for each year's own average speech length\n",
    "# byYearBar = alt.Chart(textByYear).mark_bar().encode(\n",
    "#     x='episode_date:O',\n",
    "#     y='avg_epCount:Q',\n",
    "#     color='episode_date:O'\n",
    "# )\n",
    "\n",
    "# # compare each country to the average speech length\n",
    "# compare = (byYearBar+avg).properties(width=2000)\n",
    "# compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bars for each year's own average speech length\n",
    "# byProgBar = alt.Chart(textByProg).mark_bar().encode(\n",
    "#     x='program:N',\n",
    "#     y='avg_epCount:Q',\n",
    "#     color='program:N'\n",
    "# )\n",
    "\n",
    "# # compare each country to the average speech length\n",
    "# compare = (byProgBar+avg).properties(width=2000)\n",
    "# compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bars for each year's own average speech length\n",
    "# byHostBar = alt.Chart(textByHost).mark_bar().encode(\n",
    "#     x='speaker:N',\n",
    "#     y='avg_epCount:Q',\n",
    "#     color='speaker:N'\n",
    "# )\n",
    "\n",
    "# # compare each country to the average speech length\n",
    "# compare = (byHostBar+avg).properties(width=2000)\n",
    "# compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # credit goes to Google/Runestone for creating this function\n",
    "# def score_text(text):\n",
    "#     \"\"\"\n",
    "#     Calculates and returns the sentiment score of a text string\n",
    "#     input:\n",
    "#     text (string) - text you're analyzing the sentiment of\n",
    "#     output:\n",
    "#     (int) - a score between -1 and 1. -1 is the most negative\n",
    "#     sentiment score a text can get and +1 is the most positive \n",
    "#     \"\"\" \n",
    "#     sentence_list = tokenize.sent_tokenize(text)\n",
    "#     cscore = 0.0\n",
    "#     for sent in sentence_list:\n",
    "#         ss = analyzer.polarity_scores(sent)['compound']\n",
    "#         cscore += ss\n",
    "#     return cscore / len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epScript['sentiment'] = undf.text.map(lambda t : score_text(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt.data_transformers.enable('json')\n",
    "# alt.Chart(epScript).mark_bar().encode(x=X('sentiment', bin=True), y='count()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentByEpisode = epScript.groupby(['episode_id','program','title','episode_date'])['sentiment'].mean\n",
    "# sentBySpeaker = epScript.groupby(['speaker','host_id'])['sentiment'].mean()\n",
    "# sentByHost = pd.merge(host_ids,bySpeaker,on=['speaker','host_id'],how='left')\n",
    "# sentByYear = epScript.groupby(['episode_date'].dt.year)['sentiment'].mean()\n",
    "# sentByProgram = epScript.groupby(['program'])['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charting Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find avg sentiment\n",
    "# avgSent = epScript.sentiment.mean()\n",
    "\n",
    "# # I created a dataframe that just has each year and the avergage\n",
    "# # sentiment in speeches\n",
    "# source = pd.DataFrame({\n",
    "#   'year': sentByYear.year,\n",
    "#   'avg': avgSent\n",
    "# })\n",
    "# # so that I can create a single horizontal graph that I can\n",
    "# # overlay it with the byYear graph\n",
    "# avg = alt.Chart(source).mark_rule(color='red').encode(\n",
    "#     y='avg'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # average sentiment over the years\n",
    "# byYear = alt.Chart(sentByYear).mark_line().encode(\n",
    "#     x='year:O',\n",
    "#     y='sentiment:Q',\n",
    "# )\n",
    "\n",
    "# # compare each year's sentiment to the average\n",
    "# compare = (byYear+avg)\n",
    "# compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a chart that shows where regions are in terms of starting and mid-career salaries\n",
    "# alt.Chart(typeAndRegion).mark_circle().encode(\n",
    "#     x='Starting Median Salary',\n",
    "#     y='Mid-Career Median Salary',\n",
    "#     color='School Type:N',\n",
    "#     tooltip=['School Name']\n",
    "# ).properties(\n",
    "#     width=400,\n",
    "#     height=300,\n",
    "#     title='Change in Salary by School Type'\n",
    "# ).interactive()\n",
    "\n",
    "# creates multiple heatmaps for each column in variables\n",
    "# alt.Chart(states).mark_geoshape().encode(\n",
    "#     alt.Color(alt.repeat('row'), type='quantitative')\n",
    "# ).transform_lookup(\n",
    "#     lookup='id',\n",
    "#     from_=alt.LookupData(everything, 'id', variables)\n",
    "# ).properties(\n",
    "#     width=500,\n",
    "#     height=300\n",
    "# ).project(\n",
    "#     type='albersUsa'\n",
    "# ).repeat(\n",
    "#     row=variables\n",
    "# ).resolve_scale(\n",
    "#     color='independent'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Data\n",
    "If you want to save everything created for easy use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These DataFrames take a while to create so that's why you might want to just save them once you've done it once, they do take over 3 gigabytes of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textByYear.to_csv(r'from-notebook/transcripts-by-year.csv')\n",
    "# textByProg.to_csv(r'from-notebook/transcripts-by-program.csv')\n",
    "# textByHost.to_csv(r'from-notebook/transcripts-by-host.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Counters also take a while to create so you can save them to a .txt file and read them in with the code at [Download More Ready To Go Data](#Download-More-Ready-To-Go-Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for year in textByYear.episode_date:\n",
    "#     yearCounters[year] = dict(yearCounters[year])\n",
    "\n",
    "# for program in textByProg.program:\n",
    "#     progCounters[program] = dict(progCounters[program])\n",
    "\n",
    "# for host in top10Hosts:\n",
    "#     hostCounters[host] = dict(hostCounters[host])\n",
    "\n",
    "# with open('from-notebook/yearCounters.txt', 'w') as file:\n",
    "#     json.dump(yearCounters, file)\n",
    "\n",
    "# with open('from-notebook/progCounters.txt', 'w') as file:\n",
    "#     json.dump(progCounters, file)\n",
    "\n",
    "# with open('from-notebook/hostCounters.txt', 'w') as file:\n",
    "#     json.dump(hostCounters, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are really big so you might just want to rerun the code (it doesn't take too long) rather than saving because they have the same data but rearranged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epScript.to_csv(r'from-notebook/transcripts-divided.csv')\n",
    "# bySpeaker.to_csv(r'from-notebook/transcripts-by-speaker.csv')\n",
    "# byEpisode.to_csv(r'from-notebook/transcripts-by-episode.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unnecessary to save to csv so you should ignore this unless you REALLY want to and have a lot of room on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utter.to_csv(r'from-notebook/updated-utter.csv')\n",
    "# utter2.to_csv(r'from-notebook/updated-utter2.csv')\n",
    "# episodes.to_csv(r'from-notebook/updated-episodes.csv')\n",
    "# host_ids.to_csv(r'from-notebook/updated-host_ids.csv')\n",
    "# host_map.to_csv(r'from-notebook/updated-host_map.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unused Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIDN'T END UP USING THESE BECAUSE I DIDN'T ANALYZE FREQUENCY HOW I THOUGHT I WOULD\n",
    "\n",
    "# def createTopX(x, counter):\n",
    "#     \"\"\"\n",
    "#     Creates and returns a dictionary of the top x words in counter.\n",
    "#     input:\n",
    "#     x (int) - int of the most frequent words desired\n",
    "#     counter (Counter) - Counter from which we get the words from\n",
    "#     output:\n",
    "#     (dict) - a dictionary that has x keys as the words and their\n",
    "#     corresponding values are the number of times it has appeared\n",
    "#     \"\"\" \n",
    "#     return dict(counter.most_common(x))\n",
    "\n",
    "# # mapping the two lists to the word and their count\n",
    "# def map_counts(words, counts, d):\n",
    "#     \"\"\"\n",
    "#     Maps words and their counts from a dictionary, d, to two separate lists, \n",
    "#     words and counts, in which their indices correspond with their values.\n",
    "#     input:\n",
    "#     words (list) - empty list to fill\n",
    "#     counts (list) - empty list to fill\n",
    "#     d (dict) - dictionary of words and their counts\n",
    "#     output:\n",
    "#     nothing - this function updates the already existing lists passed\n",
    "#     and does not return anything\n",
    "#     \"\"\" \n",
    "#     for word in d:\n",
    "#         words.append(word)\n",
    "#         counts.append(d[word])\n",
    "\n",
    "# # create dataframe for each x (could be year, program, host, etc.) that has the words and frequency\n",
    "# def createFreqDF(x,topX,dfName):\n",
    "#     \"\"\"\n",
    "#     Creates and returns a dictionary of the top x words in counter.\n",
    "#     input:\n",
    "#     x (object) - by what type are we recording word count by, i.e. \n",
    "#     are we creating a DataFrame that is number of words by each year,\n",
    "#     each program, host, or speaker?\n",
    "#     topX (dict) - dictionary from which words we want to include\n",
    "#     dfName (string) - allows us to return a DataFrame with this as\n",
    "#     the variable name, doesn't exist outside of this function\n",
    "#     output:\n",
    "#     (DataFrame) - a DataFrame that has 3 columns, word, count, x\n",
    "#     \"\"\" \n",
    "#     dfName = pd.DataFrame(columns=['year','word','count'])\n",
    "#     words = []\n",
    "#     counts = []\n",
    "#     map_counts(words,counts,topX[x])\n",
    "#     dfName['word'] = words\n",
    "#     dfName['count'] = counts\n",
    "#     dfName[x] = [x]*len(words)\n",
    "#     return dfName\n",
    "\n",
    "# yearTop20 = {}\n",
    "# for year in textByYear.episode_date:\n",
    "#     yearTop20[year] = createTopX(20,yearCounters[year])\n",
    "# yearFreqDF = {}\n",
    "# for year in textByYear.episode_date:\n",
    "#     name='freq'+str(year)\n",
    "#     yearFreqDF[year] = createFreqDF(year,yearTop20,name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
